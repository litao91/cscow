\chapter{Linear Models for Regression}
The outline:
\begin{itemize}
    \item Transform the input into features with basis function
        $\phi(\bx)$
    \item Linear combination of features
        \[
            y(\bx, \bw) = w_0 + \sum_{j=1}^{M-1}w_j\phi_j(\bx)
        \]
    \item Minimize the squared-error to find the parameters.
    \item Geometry of least squares: orthogonal projection of target
        vector to the feature plane.
\end{itemize}
\section{Linear Basis Function Models}
Simplest, linear combination of input:
\begin{equation}
    y(\bx, \bw) = w_0 + w_1 x_1 + \cdots + w_D x_D
\end{equation}

Extend to a linear combination of fixed nonlinear functions of input
variables:
\begin{equation}
    y(\bx, \bw) = w_0 + \sum_{j=1}^{M-1} w_j \phi_i(\bx)
\end{equation}
Where $\phi_j(\bx)$ are \textbf{basis functions}.

The parameter $w_0$ allows for any fixed offset in the data and is called
\textbf{bias} parameter.

Define an dummy basis function $\phi_0(\bx) = 1$ so that 
\begin{equation}
    y(\bx, \bw) = \sum_{j=0}^{M-1}w_j \phi_j(\bx) = \bw^T \phi(\bx)
\end{equation}
where $\bw = (w_0, \dots, w_{M-1})$ and $\boldsymbol\phi = {(\phi_0, \dots,
\phi_{M-1})}^T$

Features can be expressed in terms of basis function $\{\phi_j(\bx)\}$.

\subsection{Some choices of basis functions}
\textbf{Gaussian basis}:
\begin{equation}
    \phi_j(x) = \exp\left\{ -\frac{{(x-\mu_j)}^2}{2s^2} \right\}
\end{equation}
$\mu_j$ govern the location and $s$ governs the spatial scale.

\textbf{Sigmoidal basis} function of the form
\begin{equation}
    \phi_j(x) = \sigma\left( \frac{x-\mu_j}{s} \right)
\end{equation}
Where $\sigma(a)$ is the logistic sigmoid function:
\begin{equation}
    \sigma(a) = \frac{1}{1+\exp(-a)}
\end{equation}

\section{Maximum likelihood and least square}
Assume that target variable $t$ is given by:
\begin{equation}
    t = y(\bx, \bw) + \epsilon
\end{equation}
Where $\epsilon$ is a zero mean Gaussian random variable with precision
$\beta$ (inverse variance), works as noise. So we have
\begin{equation}
    p(t|\bx, \bw, \beta) = \mathcal{N}(t|y(\bx, \bw), \beta^{-1})
\end{equation}

Then the conditional mean will be 
\begin{equation}
    \bbE[t|\bx] = \int t p(t|\bx) dt = y(\bx, \bw)
\end{equation}

Note: the Gaussian noise assumption implies that the conditional
distribution of $t$ given $\bx$ is unimodal.

A data set of input $\mathbf{X} = \left\{ \bx_1, \dots, \bx_N \right\}$
with targets $\bsft={(t_1, \dots, t_N)}^T$ (col vector). 
\begin{equation}
    p(\bsft|\bX, \bw, \beta)= \prod_{n=1}^N \mathcal{N}(t_n|\bw^T
    \boldsymbol\phi(\bx_n), \beta^{-1})
\end{equation}

Note that in supervised learning, we are not seeking to model the
distribution of the input variables. Thus $\bx$ will always appear in the
set of conditioning variables, and so we can drop the explicit $\bx$ from
the expression.

Taking the log-likelihood:
\begin{align}
    \ln p(\bsft| \bw, \beta) &= \sum_{n=1}^N \ln
    \mathcal{N}(t_n|\bw^T\boldsymbol\phi(\bx_n), \beta^{-1})
    &=\frac{N}{2} \ln \beta - \frac{N}{2}\ln(2\pi) - \beta E_D(\bw)
    \label{lin-likelihood}
\end{align}
Where the sum-of-square error is defined by:
\begin{equation}
    E_D(\bw) = \frac{1}{2}\sum_{n=1}^N {\left\{ t_n - \bw^T
    \boldsymbol\phi(\bx_n) \right\}}^2
\end{equation}
Maximization of likelihood function under a conditional Gaussian noise
distribution for a linear model is equivalent of minimizing a
sum-of-square error function given by $E_D(\bw)$.

Take gradient of~\ref{lin-likelihood} and set to zero, we get:
\begin{equation}
    W_{ML} = {(\boldsymbol\Phi^T\boldsymbol\Phi)}^{-1}\boldsymbol\Phi^T
    \bsft
\end{equation}

Where $\boldsymbol\Phi$ is called \emph{designed matrix}, whose elements are
given by $\phi_{nj} = \phi_j(\bx_n)$
\begin{equation}
    \boldsymbol\Phi = \begin{pmatrix}
        \phi_0(\bx_1) & \phi_1(\bx_1) \cdots \phi_{M-1}(\bx_1) \\
        \phi_0(\bx_2) & \phi_1(\bx_2) \cdots \phi_{M-1}(\bx_2)\\
        \cdots \\
        \phi_0(\bx_N) & \phi_1(\bx_N) \cdots \phi_{M-1}(\bx_N) 
    \end{pmatrix}
\end{equation}

The quantity
\begin{equation}
    {(\boldsymbol\Phi^T\boldsymbol\Phi)}^{-1}\boldsymbol\Phi^T
\end{equation}
Is known as \textbf{pseudo-inverse} of the matrix.
\subsection{Geometry of least squares}
\begin{enumerate}
    \item N-dimensional space whose axes are given by $t_n$
    \item Each basis $\phi_j(\bx_n)$ can be represented as a vector in the
    same space, denoted by $\phi_j$ ($j$-th column of $\boldsymbol\Phi$)
    \item The $M$ vectors $\phi_j(\bx_n)$ will span a linear subspace
        $\mathcal{S}$ of dimensionality $M$.
    \item the least square error solution is the orthogonal projection of
        $\bsft$ to the subspace.
\end{enumerate}

\subsection{Regularized least squares}
Add regularization term to control over-fitting:
\[ E_D(\bw) + \lambda E_w(\bw)\]
Where
\[ E_w(\bw) = \frac{1}{2} \bw^T \bw\]
The total error function becomes:
\[\frac{1}{2}\sum_{n=1}^{N} {\left\{ t_n - \bw^T\phi(\bx_n) \right\}}^2 +
\frac{\lambda}{2}\bw^T\bw\]
This particular choice of regularizer is \emph{weight decay} as it
encourages weight values to decay towards zero, unless supported by the
data. Minimize it we get:
\[\bw = {(\lambda \bI + \bPhi^T\bPhi)}^{-1}\bPhi^T\bsft \]

\vspace{1cm}

A more general regularizer:
\begin{equation}
    \frac{1}{2} \sum_{n=1}^N {\left\{ t_n - \bw^T\bphi(\bx_n) \right\}}^2
    + \frac{\lambda}{2}\sum_{j=1}^M|w_j|^q
    \label{err-gen}
\end{equation}

If $\lambda$ is sufficiently large, some of the coefficients $w_j$ are
\textbf{driven to zero}, leading to a \emph{sparse} model.

To see this, Minimize~\ref{err-gen} is equivalent to:
\begin{align}
    \mbox{minimize} & E_D(\bw) = \frac{1}{2}\sum_{n=1}^N{\left\{ t_n -
        \bw^T\bphi(\bx_n)
    \right\}}^2 \\
    \mbox{subject to} & \sum_{j=1}^M |w_j|^q \leq \eta
\end{align}
The problem of determining model complexity is shifted from one of finding
the appropriate number of basis functions to one of determining a suitable
value of regularization coefficient $\lambda$

Note that $\sum_{j=1}^M |w_j|^q \leq \eta$ form a region.

\subsection{Multiple Output}
$K > 1$ target variables.


