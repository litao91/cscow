\section{Multi-Label Classification}
\subsection{HOMER Algorithm}
\subsubsection{Training}
General idea: transform large set of labels $L$ into a tree-shaped
hierarchy of simpler multi-label classification tasks.
\begin{itemize}
    \item Each node $n$ of the tree contains $L_n \subseteq L$
    \item There are $|L|$ leaves, each one containing single distinct
        label $\lambda_i \in L$
    \item Each node contains the union of the label set of its children.
        $L_{root} = L$
\end{itemize}

\emph{Meta-label}: of a node $n$, $\mu_n$ as the disjunction of the labels
contained in the node. A training example can be considered annotated with
meta-label $\mu_n$ if it is annotated with at least one of the labels in
$L_n$

Each internal node $n$ of the hierarchy also contains a multilabel
classifier $h_n$. 

Task of $h_n$: the prediction of one or more of the labels of its
children. Set of labels for $h_n$
\[
    M_n = \{ \mu_c | c \in \mbox{children}(n)\}
\]


\subsubsection{Prediction}
\begin{enumerate}
    \item Starts with $h_{root}$
    \item Follows a recursive process forwarding $x$ to the multilabel
        classifier $h_c$ of a child node $c$ only if $\mu_c$ is among the
        prediction of $h_{\mbox{parent}(c)}$
    \item Eventually, this process may lead to the prediction of one or
        more single-labels by the multi-label classifiers.
    \item The union of these predicted single-labels is the output of hte
        proposed approach in this case.
\end{enumerate}
\subsubsection{Training}
\begin{enumerate}
    \item Assume the existence of a set $D = \{(\bx_i, \bY_i) | i = 1
        \cdots |D|\}$, each one consists of a feature vector $\bx_i$ and
        set of labels $\bY_i \subseteq L$.
    \item Recursively in a top-down depth-first fashion starting with the
        root
    \item At each $n$, $k$ children nodes are first created, unless $|L_n|
        < k$, in which the number of children is $|L_n|$.
    \item Each such child $n$ filters the data of its parent, keeping only
        the eexample that are annotated with at least one of its own
        labels.
    \item Two main processes are then sequentially executed:
        \begin{enumerate}
            \item labels of the current node are distributed into $k$
                disjoint subsets, one for each child of the current node
            \item A multilabel classifier is trained for the prediction of
                the meta-labels of its children.
        \end{enumerate}
    \item The approach recurses into each child node that contains no more
        than a single label.
\end{enumerate}

Grouping: balanced k-means.

\subsection{RAKEL}
Let $L = \{\lambda_i\}, i = 1 \dots |L|$ be the set of labels. A set
$Y\subseteq L$ with $k = |Y|$ is called $k$-labelset.

\subsection{MLkNN}
\subsubsection{Preliminaries}
Notations
\begin{description}
    \item[domain of instances] $\mathcal{X}$
    \item[Finite set of labels] $\mathcal{Y} = \{(x_1, Y_1), (x_2, Y_2),
        \dots, (x_m, Y_m)\}, (x_i \in \mathcal{X}, Y_i \subseteq
        \mathcal{Y})$
\end{description}

Learning system will produce a real-valued function of the form:
\[ f: \mathcal{X} \times \mathcal{Y} \rightarrow \mathcal{R}\]

A successful learning system will tend to output larger values for labels
in $Y_i$ than those not in $Y_i$. That is
\[ f(x_i, y_1) > f(x_i, y_2), \forall y_1 \in Y_i, \mbox{and} y_2 \not\in
Y_i\]

The corresponding multi-label classifier $h(\cdot)$:
\[ h(x_i) = \{ y|f(x_i, y) > t(x_i), y \in \mathcal{Y}\}\]
Where $t(\cdot)$ is a threshold function which is usually set to be the
zero constant.

\subsection{ML-KNN}

Given an instance $x$ and its associated label set $Y\in \mathcal{Y}$

Let $\vec{y_x}$ be the category vector for $x$:
\[
    \vec{y_x}(l) = 
    \begin{cases}
    1 & l \in Y \\
    0 & \mbox{otherwise}
    \end{cases}
\]

Let $N(x)$ denote the set of the label sets of these neighbors, a
\emph{membership counting vector} can be defined as:

\[ \vec{C}(x) = \sum_{a \in N(x)}\vec{y_a}(l), l \in \mathcal{Y}\]

$\vec{C_x}(l)$ counts the number of neighbors of $x$ belonging to the
$l$th class.

\begin{itemize}
    \item  For each test instance $t$, ML-KNN firstly identifies its KNNs $N(t)$ in
        the training set.
    \item $N(t)$ --- neighbors of $t$
    \item $\vec{C}_t(l)$ --- membership counting vector. Number of element
        in $N(t)$ that has label $l$.
    \item $H_b^l$, $b \in {0, 1}$ --- $t$ (the testing instance) has label
        $l$ if $b = 1$, otherwise if $b = 0$
    \item $\vec{y}_x(l)$ --- $1$ if instance $x$ has label $l$
    \item $E_j^l, j \in \{0, 1, \dots, K\}$ --- Among $N(t)$ exactly $j$ instances have label $l$.
\end{itemize}

Based on the membership counting vector $\vec{C_t}$, the category vector
$\vec{y_t}$ is determined using the following MAP principle:

\[ 
    \vec{y}_t(l) = \arg\max_{b\in \{0, \}} P(H_b^l|E_{\vec{C}_t(l)}^l), l
    \in \mathcal{Y}
\]

Using the Bayesian rule:
\begin{align*}
    \vec{y}_t(l) &= \arg\max_{b\in \{0, 1\}} \frac{P(H_b^l)
        P(E^l_{\vec{C}_t(l)}|H_b^l)}{P(E_{\vec{C}_t(l)}^l|H_b^l)} \\
        &= \arg\max_{b \in \{0, 1\}} P(H_b^l)P(E_{\vec{C}_t(l)}^l|H_b^l)
\end{align*}

Algorithm
\begin{enumerate}
    \item For each label $l \in \cY$, calculate the prior:
        \[P(H_1^l) = \frac{(s + \sum_{i=1}^m y_{x_i}(l))}{s \times 2 +
        m}\]
        The probability that any instance has label $l$. $s$ is smoothing
        parameter. It is basically $\frac{\#\mbox{instance has
        }l}{\#\mbox{total training instances}}$.
        \[ P(H_0^l) = 1 - P(H_1^l)\]
    \item Identify the KNN $N(x_i), i \in \{1, 2, \dots, m\}$ ($m$ testing
        instances)
    \item for each label $l \in \cY$, do the following:
        \begin{enumerate}
            \item For $j \in \left\{ 0, 1,
                \dots, K \right\}$ (the possible count of labels) initialize the counting $c[j] = 0$ and
                $c'[j] = 0$
            \item For each of the training instances $i \in \left\{ 1, 2,
                \dots m \right\}$, the number of label $l$ in the
                neighbors of $x_i$:
                \[ \delta = C_{x_i}^(l) \leftarrow \sum_{a\in
                N(x_i)\vec{y}_a(l)}\]

                If $\vec{y}_{x_i}(l) =  1$, that is, if instance $x_i$
                has label $l$,  then $c[\delta] \leftarrow
                c[\delta] + 1$, else $c'[\delta] \leftarrow c'[\delta]
                + 1$.

                So $c[\delta]$ is the cases that the training instance
                $x_i$ has label $l$ and among its neighbors, there are exactly
                $\delta$ instances also have label $l$.
            \item Calculate the posterior probabilities $P(E_j^l|H_b^l)$,
                probability that produce $E_j^l$ ($j$ label $l$ in
                the neighbor) given the instance $H_b^l$ (has or does not
                have label $l$)
                The probability that $l$ has $j$ instances
                \[
                    P(E_j^l|H_1^l) = \frac{s + c[j]}{s\times{K+1} +
                    \sum_{p=0}^k c[p]}
                \]
                In words, it is (ignoring the smoothing terms):
                \[ P(\mbox{$j$ neighbors with
                label $l$}) = \frac{\#\mbox{num instances with $l$ has
                $j$ neighbors labelled $l$ neighbors}}{\#\mbox{total
                    number of $l$ instances whose
                neighbors has label $l$}}
            \]
        \end{enumerate}
\end{enumerate}

