\section{Multi-Label Classification}
\subsection{HOMER Algorithm}
\subsubsection{Training}
General idea: transform large set of labels $L$ into a tree-shaped
hierarchy of simpler multi-label classification tasks.
\begin{itemize}
    \item Each node $n$ of the tree contains $L_n \subseteq L$
    \item There are $|L|$ leaves, each one containing single distinct
        label $\lambda_i \in L$
    \item Each node contains the union of the label set of its children.
        $L_{root} = L$
\end{itemize}

\emph{Meta-label}: of a node $n$, $\mu_n$ as the disjunction of the labels
contained in the node. A training example can be considered annotated with
meta-label $\mu_n$ if it is annotated with at least one of the labels in
$L_n$

Each internal node $n$ of the hierarchy also contains a multilabel
classifier $h_n$. 

Task of $h_n$: the prediction of one or more of the labels of its
children. Set of labels for $h_n$
\[
    M_n = \{ \mu_c | c \in \mbox{children}(n)\}
\]


\subsubsection{Prediction}
\begin{enumerate}
    \item Starts with $h_{root}$
    \item Follows a recursive process forwarding $x$ to the multilabel
        classifier $h_c$ of a child node $c$ only if $\mu_c$ is among the
        prediction of $h_{\mbox{parent}(c)}$
    \item Eventually, this process may lead to the prediction of one or
        more single-labels by the multi-label classifiers.
    \item The union of these predicted single-labels is the output of hte
        proposed approach in this case.
\end{enumerate}
\subsubsection{Training}
\begin{enumerate}
    \item Assume the existence of a set $D = \{(\bx_i, \bY_i) | i = 1
        \cdots |D|\}$, each one consists of a feature vector $\bx_i$ and
        set of labels $\bY_i \subseteq L$.
    \item Recursively in a top-down depth-first fashion starting with the
        root
    \item At each $n$, $k$ children nodes are first created, unless $|L_n|
        < k$, in which the number of children is $|L_n|$.
    \item Each such child $n$ filters the data of its parent, keeping only
        the eexample that are annotated with at least one of its own
        labels.
    \item Two main processes are then sequentially executed:
        \begin{enumerate}
            \item labels of the current node are distributed into $k$
                disjoint subsets, one for each child of the current node
            \item A multilabel classifier is trained for the prediction of
                the meta-labels of its children.
        \end{enumerate}
    \item The approach recurses into each child node that contains no more
        than a single label.
\end{enumerate}

Grouping: balanced k-means.

\subsection{RAKEL}
Let $L = \{\lambda_i\}, i = 1 \dots |L|$ be the set of labels. A set
$Y\subseteq L$ with $k = |Y|$ is called $k$-labelset.

\subsection{MLkNN}
\subsubsection{Preliminaries}
Notations
\begin{description}
    \item[domain of instances] $\mathcal{X}$
    \item[Finite set of labels] $\mathcal{Y} = \{(x_1, Y_1), (x_2, Y_2),
        \dots, (x_m, Y_m)\}, (x_i \in \mathcal{X}, Y_i \subseteq
        \mathcal{Y})$
\end{description}

Learning system will produce a real-valued function of the form:
\[ f: \mathcal{X} \times \mathcal{Y} \rightarrow \mathcal{R}\]

A successful learning system will tend to output larger values for labels
in $Y_i$ than those not in $Y_i$. That is
\[ f(x_i, y_1) > f(x_i, y_2), \forall y_1 \in Y_i, \mbox{and} y_2 \not\in
Y_i\]

The corresponding multi-label classifier $h(\cdot)$:
\[ h(x_i) = \{ y|f(x_i, y) > t(x_i), y \in \mathcal{Y}\}\]
Where $t(\cdot)$ is a threshold function which is usually set to be the
zero constant.

\subsection{ML-KNN}

Given an instance $x$ and its associated label set $Y\in \mathcal{Y}$

Let $\vec{y_x}$ be the category vector for $x$:
\[
    \vec{y_x}(l) = 
    \begin{cases}
    1 & l \in Y \\
    0 & \mbox{otherwise}
    \end{cases}
\]

Let $N(x)$ denote the set of the label sets of these neighbors, a
\emph{membership counting vector} can be defined as:

\[ \vec{C}(x) = \sum_{a \in N(x)}\vec{y_a}(l), l \in \mathcal{Y}\]

$\vec{C_x}(l)$ counts the number of neighbors of $x$ belonging to the
$l$th class.

\begin{itemize}
    \item  For each test instance $t$, ML-KNN firstly identifies its KNNs $N(t)$ in
        the training set.
    \item Let $H_1^l$ be the event that $t$ has label $l$, while $H_0^l$
        be the event that $t$ has no label $l$.
    \item Let $E_j^l (j \in \{0, 1, \dots, K\})$ denote the event that,
        among the KNNs of $t$, there are exactly $j$ instances which have
        label $l$. 
\end{itemize}

Based on the membership counting vector $\vec{C_t}$, the category vector
$\vec{y_t}$ is determined using the following MAP principle:

\[ 
    \vec{y}_t(l) = \arg\max_{b\in \{0, \}} P(H_b^l|E_{\vec{C}_t(l)}^l), l
    \in \mathcal{Y}
\]

Using the Bayesian rule:
\begin{align*}
    \vec{y}_t(l) &= \arg\max_{b\in \{0, 1\}} \frac{P(H_b^l)
        P(E^l_{\vec{C}_t(l)}|H_b^l)}{P(E_{\vec{C}_t(l)}^l|H_b^l)} \\
        &= \arg\max_{b \in \{0, 1\}} P(H_b^l)P(E_{\vec{C}_t(l)}^l|H_b^l)
\end{align*}

Algorithm

