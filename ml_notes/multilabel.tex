\chapter{Multi-Label Classification}
\section{HOMER Algorithm}
\subsection{Training}
General idea: transform large set of labels $L$ into a tree-shaped
hierarchy of simpler multi-label classification tasks.
\begin{itemize}
    \item Each node $n$ of the tree contains $L_n \subseteq L$
    \item There are $|L|$ leaves, each one containing single distinct
        label $\lambda_i \in L$
    \item Each node contains the union of the label set of its children.
        $L_{root} = L$
\end{itemize}

\emph{Meta-label}: of a node $n$, $\mu_n$ as the disjunction of the labels
contained in the node. A training example can be considered annotated with
meta-label $\mu_n$ if it is annotated with at least one of the labels in
$L_n$

Each internal node $n$ of the hierarchy also contains a multilabel
classifier $h_n$. 

Task of $h_n$: the prediction of one or more of the labels of its
children. Set of labels for $h_n$
\[
    M_n = \{ \mu_c | c \in \mbox{children}(n)\}
\]


\subsection{Prediction}
\begin{enumerate}
    \item Starts with $h_{root}$
    \item Follows a recursive process forwarding $x$ to the multilabel
        classifier $h_c$ of a child node $c$ only if $\mu_c$ is among the
        prediction of $h_{\mbox{parent}(c)}$
    \item Eventually, this process may lead to the prediction of one or
        more single-labels by the multi-label classifiers.
    \item The union of these predicted single-labels is the output of hte
        proposed approach in this case.
\end{enumerate}
\subsection{Training}
\begin{enumerate}
    \item Assume the existence of a set $D = \{(\bx_i, \bY_i) | i = 1
        \cdots |D|\}$, each one consists of a feature vector $\bx_i$ and
        set of labels $\bY_i \subseteq L$.
    \item Recursively in a top-down depth-first fashion starting with the
        root
    \item At each $n$, $k$ children nodes are first created, unless $|L_n|
        < k$, in which the number of children is $|L_n|$.
    \item Each such child $n$ filters the data of its parent, keeping only
        the eexample that are annotated with at least one of its own
        labels.
    \item Two main processes are then sequentially executed:
        \begin{enumerate}
            \item labels of the current node are distributed into $k$
                disjoint subsets, one for each child of the current node
            \item A multilabel classifier is trained for the prediction of
                the meta-labels of its children.
        \end{enumerate}
    \item The approach recurses into each child node that contains no more
        than a single label.
\end{enumerate}

Grouping: balanced k-means.

\section{RAKEL}
Let $L = \{\lambda_i\}, i = 1 \dots |L|$ be the set of labels. A set
$Y\subseteq L$ with $k = |Y|$ is called $k$-labelset.

\section{MLkNN}
\subsection{Preliminaries}
Notations
\begin{description}
    \item[domain of instances] $\mathcal{X}$
    \item[Finite set of labels] $\mathcal{Y} = \{(x_1, Y_1), (x_2, Y_2),
        \dots, (x_m, Y_m)\}, (x_i \in \mathcal{X}, Y_i \subseteq
        \mathcal{Y})$
\end{description}

Learning system will produce a real-valued function of the form:
\[ f: \mathcal{X} \times \mathcal{Y} \rightarrow \mathcal{R}\]

A successful learning system will tend to output larger values for labels
in $Y_i$ than those not in $Y_i$. That is
\[ f(x_i, y_1) > f(x_i, y_2), \forall y_1 \in Y_i, \mbox{and} y_2 \not\in
Y_i\]

The corresponding multi-label classifier $h(\cdot)$:
\[ h(x_i) = \{ y|f(x_i, y) > t(x_i), y \in \mathcal{Y}\}\]
Where $t(\cdot)$ is a threshold function which is usually set to be the
zero constant.

\subsection{ML-KNN}

Given an instance $x$ and its associated label set $Y\in \mathcal{Y}$

Let $\vec{y_x}$ be the category vector for $x$:
\[
    \vec{y_x}(l) = 
    \begin{cases}
    1 & l \in Y \\
    0 & \mbox{otherwise}
    \end{cases}
\]

Let $N(x)$ denote the set of the label sets of these neighbors, a
\emph{membership counting vector} can be defined as:

\[ \vec{C}(x) = \sum_{a \in N(x)}\vec{y_a}(l), l \in \mathcal{Y}\]

$\vec{C_x}(l)$ counts the number of neighbors of $x$ belonging to the
$l$th class.

\begin{itemize}
    \item  For each test instance $t$, ML-KNN firstly identifies its KNNs $N(t)$ in
        the training set.
    \item $N(t)$ --- neighbors of $t$
    \item $\vec{C}_t(l)$ --- membership counting vector. Number of element
        in $N(t)$ that has label $l$.
    \item $H_b^l$, $b \in {0, 1}$ --- $t$ (the testing instance) has label
        $l$ if $b = 1$, otherwise if $b = 0$
    \item $\vec{y}_x(l)$ --- $1$ if instance $x$ has label $l$
    \item $E_j^l, j \in \{0, 1, \dots, K\}$ --- Among $N(t)$ exactly $j$ instances have label $l$.
\end{itemize}

Based on the membership counting vector $\vec{C_t}$, the category vector
$\vec{y_t}$ is determined using the following MAP principle:

\[ 
    \vec{y}_t(l) = \arg\max_{b\in \{0, \}} P(H_b^l|E_{\vec{C}_t(l)}^l), l
    \in \mathcal{Y}
\]

Using the Bayesian rule:
\begin{align*}
    \vec{y}_t(l) &= \arg\max_{b\in \{0, 1\}} \frac{P(H_b^l)
        P(E^l_{\vec{C}_t(l)}|H_b^l)}{P(E_{\vec{C}_t(l)}^l|H_b^l)} \\
        &= \arg\max_{b \in \{0, 1\}} P(H_b^l)P(E_{\vec{C}_t(l)}^l|H_b^l)
\end{align*}

Algorithm
\begin{enumerate}
    \item For each label $l \in \cY$, calculate the prior:
        \[P(H_1^l) = \frac{(s + \sum_{i=1}^m y_{x_i}(l))}{s \times 2 +
        m}\]
        The probability that any instance has label $l$. $s$ is smoothing
        parameter. It is basically $\frac{\#\mbox{instance has
        }l}{\#\mbox{total training instances}}$.
        \[ P(H_0^l) = 1 - P(H_1^l)\]
    \item Identify the KNN $N(x_i), i \in \{1, 2, \dots, m\}$ ($m$ testing
        instances)
    \item for each label $l \in \cY$, do the following:
        \begin{enumerate}
            \item For $j \in \left\{ 0, 1,
                \dots, K \right\}$ (the possible count of labels) initialize the counting $c[j] = 0$ and
                $c'[j] = 0$
            \item For each of the training instances $i \in \left\{ 1, 2,
                \dots m \right\}$, the number of label $l$ in the
                neighbors of $x_i$:
                \[ \delta = C_{x_i}^(l) \leftarrow \sum_{a\in
                N(x_i)\vec{y}_a(l)}\]

                If $\vec{y}_{x_i}(l) =  1$, that is, if instance $x_i$
                has label $l$,  then $c[\delta] \leftarrow
                c[\delta] + 1$, else $c'[\delta] \leftarrow c'[\delta]
                + 1$.

                So $c[\delta]$ is the cases that the training instance
                $x_i$ has label $l$ and among its neighbors, there are exactly
                $\delta$ instances also have label $l$.
            \item Calculate the posterior probabilities $P(E_j^l|H_b^l)$,
                probability that produce $E_j^l$ ($j$ label $l$ in
                the neighbor) given the instance $H_b^l$ (has or does not
                have label $l$)
                The probability that $l$ has $j$ instances
                \[
                    P(E_j^l|H_1^l) = \frac{s + c[j]}{s\times{K+1} +
                    \sum_{p=0}^k c[p]}
                \]
                In words, it is (ignoring the smoothing terms):
                \[ P(\mbox{$j$ neighbors with
                label $l$}) = \frac{\#\mbox{num instances with $l$ has
                $j$ neighbors labelled $l$ neighbors}}{\#\mbox{total
                    number of $l$ instances whose
                neighbors has label $l$}}
            \]
        \end{enumerate}
\end{enumerate}

\section{Label Partition}
A data set of pairs $(x_i, y_i)$, $i = 1, \dots, m$. Possible labels
$\mathcal{D}$.

Goal: given a new example $x^*$, to rank the entire set of labels
$\mathcal{D}$ and output the top $k$ to the user which should contain the
most relevant results possible.

It is assumed that user has already trained a label scorer $f(x,y)$ that
for a given label returns a real-valued score.

Two components:
\begin{itemize}
    \item An \emph{input partitioner} that given an input example, maps it
        to one or more partitions of the input space.
    \item \emph{label assignment} which assigns a subset of labels to each
        partition.
\end{itemize}

For given example, the label scorer is applied to only the subset of
labels present in the corresponding partitions.

At prediction time:

\begin{enumerate}
    \item Given a test input $x$, the input partitioner maps $x$ to a set
        of partitions $p = g(x)$.
    \item We retrieve the label sets assigned to each partition $p_j: L =
        \cup_{j=1}^{|p|} \mathcal{L}_{p_j}, \quad p_j \in p$ where $\mathcal{L}_{p_j}
        \subseteq \mathcal{D}$ is the subset of labels assigned to
        partition $p_j$
    \item We score the labels $y \in L$ with the label scorer $f(x,y)$,
        and rank them to produce our final result.
\end{enumerate}

\subsection{Input Partitioner}
$g(x) \rightarrow p \subseteq \mathcal{P}$o
Where there are $P$ possible partitions, $\mathcal{P} = \left\{ 1, \dots P
\right\}$

Goal: to partition the input space such that examples that have the same
relevant labels highly ranked by the label scorer are in the same
partition.

For given example $(x_i, y_i)$, the accuracy:
\[\hat{l}(f(x_i), y_i)\]

The loss is to be minimized as (equivalent to max accuracy):
\begin{equation}
    l(f(x_i), y_i) = 1- \hat{l}(f(x_i), y_i)
\end{equation}

Here $f(x)$ is the vector of scores for all labels:
\[ f(x) = f_{\mathcal{D}}(x) = \left( f(x, \mathcal{D}_1), \dots,
f(x, \mathcal{D}_{|\mathcal{D}|}) \right)\]
where $\mathcal{D}_i$ is the $i$th label.

To measure the loss of partitioner:
\begin{equation}
    l(f_{g(x_i)}(x_i), y_i)
\end{equation}
Where $f_{g(x)}(x) = \left( f(x, L_1), \dots, f(x, L_{|L|}) \right)$

That is, the loss within it's partition.

Then the overall loss:
\begin{equation}
    \sum_{i=1}^m l(f_{g(x_i)}(x_i), y_i)
\end{equation}

The sum of loss of all data in their partition.

The label assignment (to partitions) $\mathcal{L}$ are unknown making the
computation above infeasible. 

The errors incurred by this model can be decomposed into components. For
given example, it receives a low or zero precision at $k$ if either

\begin{itemize}
    \item It is in a partition where the relevant labels are not in the
        set
    \item The original label scorer was doing poorly in the first place.
\end{itemize}

Guidelines:

\begin{itemize}
    \item Examples that share highly relevant labels should be mapped into
        the same partition
    \item Examples for which the label scorer performs well should be
        prioritized when learning a partitioner.
\end{itemize}

Consider the case of a partitioner that works by using the closest
assigned partition as defined by partition centroids:
$c_i$, $i = 1, \dots, P$:
\begin{equation}
    g(x) = argmin_{i=1,\dots, P}\|x - c_i\|
\end{equation}

\paragraph{Weighted Hierarchical Partitioner}
Ensuring the input partitioner prioritizes examples which already perform
well with the given label scorer is to weight each training example with
tis label scorer result:
\begin{equation}
    \sum_{i=1}^m\sum_{j=1}^P \hat{l}(f(x_i), y_i) \|x_i - c_j\|^2
\end{equation}

So if the precision of the scorer with that particular instance is high,
then the distance of that instance weights more.
