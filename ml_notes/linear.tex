\chapter{Linear Models for Regression}
\section{Linear Basis Function Models}
Simplest, linear combination of input:
\begin{equation}
    y(\bx, \bw) = w_0 + w_1 x_1 + \dots + w_D x_D
\end{equation}

Extend to a linear combination of fixed nonlinear functions of input
variables:
\begin{equation}
    y(\bx, \bw) = w_0 + \sum_{j=1}^{M-1} w_j \phi_i(\bx)
\end{equation}
Where $\phi_j(\bx)$ are \textbf{basis functions}.

The parameter $w_0$ allows for any fixed offset in the data and is called
\textbf{bias} parameter.

Define an dummy basis function $\phi_0(\bx) = 1$ so that 
\begin{equation}
    y(\bx, \bw) = \sum_{j=0}^{M-1}w_j \phi_j(\bx) = \bw^T \phi(\bx)
\end{equation}
where $\bw = (w_0, \dots, w_{M-1})$ and $\boldsymbol\phi = (\phi_0, \dots,
\phi_{M-1})^T$

Features can be expressed in terms of basis function $\{\phi_j(\bx)\}$.

\subsection{Some choices of basis functions}
Gaussian basis:
\begin{equation}
    \phi_j(x) = \exp\left\{ -\frac{(x-\mu_j)^2}{2s^2} \right\}
\end{equation}
$\mu_j$ govern the location and $s$ governs the spatial scale.

Sigmoidal basis function of the form
\begin{equation}
    \phi_j(x) = \sigma\left( \frac{x-\mu_j}{s} \right)
\end{equation}
Where $\sigma(a)$ is the logistic sigmoid function:
\begin{equation}
    \sigma(a) = \frac{1}{1+\exp(-a)}
\end{equation}

\section{Maximum likelihood and least square}
Assume that target variable $t$ is given by:
\begin{equation}
    t = y(\bx, \bw) + \epsilon
\end{equation}
Where $\epsilon$ is a zero mean Gaussian random variable with precision
$\beta$ (inverse variance), works as noise. So we have
\begin{equation}
    p(t|\bx, \bw, \beta) = \mathcal{N}(t|y(\bx, \bw), \beta^{-1})
\end{equation}

Then the conditional mean will be 
\begin{equation}
    \bbE[t|\bx] = \int t p(t|\bx) dt = y(\bx, \bw)
\end{equation}

Note: the Gaussian noise assumption implies that the conditional
distribution of $t$ given $\bx$ is unimodal.

A data set of input $\mathbf{X} = \left\{ \bx_1, \dots, \bx_N \right\}$
with targets $\mathbf{t}=(t_1, \dots, t_N)^T$ (col vector). 
\begin{equation}
    p(\mathbf{t}|\bX, \bw, \beta)= \prod_{n=1}^N \mathcal{N}(t_n|\bw^T
    \boldsymbol\phi(\bx_n), \beta^{-1})
\end{equation}

Note that in supervised learning, we are not seeking to model the
distribution of the input variables. Thus $\bx$ will always appear in the
set of conditioning variables, and so we can drop the explicit $\bx$ from
the expression.

Taking the log-likelihood:
\begin{align}
    \ln p(\mathbf{t}| \bw, \beta) &= \sum_{n=1}^N \ln
    \mathcal{N}(t_n|\bw^T\boldsymbol\phi(\bx_n), \beta^{-1})
    &=\frac{N}{2} \ln \beta - \frac{N}{2}\ln(2\pi) - \beta E_D(\bw)
    \label{lin-likelihood}
\end{align}
Where the sum-of-square error is defined by:
\begin{equation}
    E_D(\bw) = \frac{1}{2}\sum_{n=1}^N \left\{ t_n - \bw^T
    \boldsymbol\phi(\bx_n) \right\}^2
\end{equation}
Maximization of likelihood function under a conditional Gaussian noise
distribution for a linear model is equivalent of minimizing a
sum-of-square error function given by $E_D(\bw)$.

Take gradient of ~\ref{lin-likelihood} and set to zero, we get:
\begin{equation}
    W_{ML} = (\boldsymbol\Phi^T\boldsymbol\Phi)^{-1}\boldsymbol\Phi^T
    \mathbf{t}
\end{equation}

Where $\boldsymbol\Phi$ is called \emph{designed matrix}, whose elements are
given by $\phi_{nj} = \phi_j(\bx_n)$
\begin{equation}
    \boldsymbol\Phi = \begin{pmatrix}
        \phi_0(\bx_1) & \phi_1(\bx_1) \cdots \phi_{M-1}(\bx_1) \\
        \phi_0(\bx_2) & \phi_1(\bx_2) \cdots \phi_{M-1}(\bx_2)\\
        \cdots \\
        \phi_0(\bx_N) & \phi_1(\bx_N) \cdots \phi_{M-1}(\bx_N) 
    \end{pmatrix}
\end{equation}

The quantity
\begin{equation}
    (\boldsymbol\Phi^T\boldsymbol\Phi)^{-1}\boldsymbol\Phi^T
\end{equation}
Is known as \textbf{pseudo-inverse} of the matrix .
\subsection{Geometry of least squares}
\begin{enumerate}
    \item N-dimensional space whose axes are given by $t_n$
    \item Each basis $\phi_j(\bx_n)$ can be represented as a vector in the
    same space, denoted by $\phi_j$ ($j$-th column of $\boldsymbol\Phi$
    \item The $M$ vectors $\phi_j(\bx_n)$ will span a linear subspace
        $\mathcal{S}$ of dimensionality $M$.
    \item the least square error solution is the orthogonal projection of
        $\mathbf{t}$ to the subspace.
\end{enumerate}

\subsection{Regularized least squares}

\chapter{Linear Model for Classification}
\section{Linear Discriminant functions}
A discriminant is a function that take an input vector $\bx$ and assigns
it to one of $K$ classes:
\[g_i(\bx|\bw_i, w_{i0}) =\bw_i^T\bx + w_{i0}\] generalized use base
function $g_i(\bx) = \sum_{j=1}^k w_j \phi_{ij}(\bx)$
\subsection{Two classes}
\[g(\bx) = g_1(\bx) - g_2(\bx) \], $C_1$ if $g(\bx)>0$

\subsection{Geometry Interpretation}
The decision boundary is defined by 
\[ g(\bx) = 0\]
Corresponds to a $(D-1)$ dimensional hyperplane within the $D$-dimensional
space.

    Express any point as 
    \[\bx = \bx_p  + r\frac{\bw}{\|\bw\|}\]
    where $x_p$ is the  projection of $\bx$ onto hyperplane. $r$ distance
    from $\bx$ to hyper plane.plane, we have $r=\frac{g(\bx)}{\|\bw\|}$
\subsection{Multi classes} 
K discriminant:
\[g_i(\bx |\bw_i, w_{i0}) = \bw_i^T + w_{i0}\]
Linear separable: \[g_i(\bx|\bw_i, \bw_{i0}) > 0\] if $\bx \in
        C_i$
        
        Choose $C_i$ if \[g_i(\bx) = \max_{j=1}^Kg_i(\bx)\]

\subsection{Pairwise Separation} 
    Discriminant function for class $i$ and $j$:
        \[
            g_{ij} (\x|\bw_{ij}, w_{ij0}) = \bw_{ij}^T\x + w_{ij0} =
            \begin{cases}
                > 0 & \mbox{if } \x \in C_i\\
                \leq 0 & \mbox{if } \x \in C_j\\
                \mbox{don't care} & \mbox{if } \x \in C_k, k\neq i, k\neq j
            \end{cases} \]
\section{Logistic Discrimination}
\subsection{Two classes}
     Assume that the log likelihood ratio is linear:
        \[\log\frac{p(\x|C_1)}{p(\x|C_2)} = \bw^T\x + w_0^o \]

    Using Baye's rule we have;
        \begin{align*}
            logit(P(C_1|x)) & = \log\frac{p(C_1|\x)}{1 - p(C_1|\x)} \\
            & = \log\frac{p(\x|C_1)}{p(\x|C_2)} + \log\frac{p(C_1)}{p(C_2)} \\
            & = \bw^T\x + w_0
        \end{align*}
        where $w_0 = w_0^o + \log\frac{P(C_1)}{P(C_2)}$

       Rearranging terms: 
        \begin{align*} y  & = sigmoid(\bw^T\x + w_0) \\
            & =  \hat{P}(C_1|\x) = \frac{1}{1 + \exp\left[-(\bw^T\x + w_0 )\right]}
        \end{align*}
        As our estimator of $P(C_1|\x)$

        \subsubsection{Gradient Decent}
    In the discriminant-based approach, the parameters are those of the
        discriminants, and they are \emph{optimized to minimize the
        classification error}
        \begin{description}
    \item[Error]$\bw$ denotes the set of parameters and $E(\bw|\cX)$ is the
        error parameters $\bw$ on the given training set 
        $\cX$, we look for: 
        \[ \bw*= arg\min_{\bw}E(\bw|\cX)\]
        No analytical solution
    \item[Gradient Vector] When $E(\bw)$ is a differentiable function of a
        vector of variables, we have the gradient vector composed of the
           partial derivatives:
            \[ \nabla_\bw E = \left[ \frac{\partial E}{\partial w_1},
                \frac{\partial E}{\partial w_2}, \dots, \frac{\partial
                E}{\partial w_d} \right]^T
                \]
            \item [Gradient Descent] starts from a \emph{random} $\bw$, at each
                step, update $w$ in a \emph{opposite direction} of the gradient:
                \[\Delta w_i = - \eta\frac{\partial E}{\partial w_i}, \forall i
                    \]
                \[ w_i = w_i + \Delta w_i \]
                $\eta$ is \emph{step size}, or \emph{learning factor}

                When we get to minimum, the derivative is 0 and the procedure
                terminates.

            \item This indicates that the procedure finds the nearest minimum
                that can be \emph{local minimum}. There is no guarantee of
                finds the nearest minimum that can be a local minimum
    \item [Learning parameters]
        Given a sample of two classes, $\mathcal{X} = {\x^{(l)}, \r^{(l)}}$, where $\r^{(l)}
        = 1$ if $\x\in C_1$

        We assume $\r^{(l)}$, given $\x^{(l)}$ is Bernoulli with probability $y^{(l)} =
        p(C_1|\x^{(l)})$ :
        \[\r^{(l)}|\x^{(l)} \sim Bernoulli(y^{(l)}) \]
        Note that in this discriminant-based approach, we model directly $\r|\x$
        The sample likelihood is:
        \[ L(\bw,w_0|\cX) = \prod_t(y^{(l)})^{(r^{(l)})}(1-y^{(l)})^{1-r^{(l)}}
            \]
        We can always turn it in an error function to minimize: $E = -\log L$
        So we have \emph{cross-entropy}:
        \[ E(\bw, w_0| \cX) = -\sum_t \r^{(l)}\log y^{(l)} + (1-r^{(l)})\log(1 - y^{(l)}) \]

        We use gradient descent to minimize cross-entropy 
        If $y = sigmoid(a) = \frac{1}{1 + exp(-a)}$, its derivative is given as:
        \[ \frac{dy}{da} = y(1-y) \]
        and we get the following update equations:
        \begin{align*}
            \Delta w_j & = -\eta \frac{\partial E}{\partial w_j} = \eta \sum_t
            ({\frac{r^{(l)}}{y^{(l)}}} - \frac{1-r^{(l)}}{1-y^{(l)}}x^{(l)}_j \\
            & = \eta \sum(r^{(l)} - y^{(l)}) x^{(l)}_j\mbox{, }= 1,\dots, d 
        \end{align*}
        \[
            \Delta w_0 = -\eta \frac{\partial E}{\partial w_0} = \eta\sum(r^{(l)} -
            y^{(l)}) 
        \]
\end{description}
\subsection{Multiple Classes}
\subsubsection{Generalization of sigmoid}
    Take one of the classes $C_k$, as reference class
        and assume that: \\
        $ \log\frac{p(\x|C_i)}{p(\x|C_K)} = \bw_i^T\x+ w_{i0}^o $,
        $i=1,\dots,K-1$\\
        Then we have:
        \[ \frac{P(C_i|\x)}{P(C_K|\x)} = \exp[\bw_i^T\x+w_{i0}] \]
        With $w_{i0} = w_{i0}^o + \log\frac{P(C_i)}{P(C_K)}$

     Summing over $i$ we can deduce: 
         \[P(C_K|\bx) =
         \frac{1}{1+\sum_{i=1}^{K-1}\exp(\bw_i^T\bx + w_{i0})}\]
        \[P(C_i|\bx)
        =\frac{\exp(\bw_i^T+w_{i0})}{1+\sum_{j=1}^{K-1}\exp(\bw_j^T\bx+w_{j0})}\],
    where $k=1,\dots, K-1$
\subsubsection{Softmax}
    Treat all classes uniformly\\ $y_i = \hat{P}(C_i|\bx) =
        \frac{\exp(\bw_i^T+w_{i0})}{\sum_{j=1}^K\exp(\bw_j^T+w_{j0})}$,
        $i=1,\dots, K$
    \paragraph{Learning} 
        \[\frac{\partial y_i}{\partial a_j} = y_i (\delta_{ij}-y_i)\]
        $\Delta\bw_j = \eta\sum_l(r_j^{(l)}-y_j^{(l)})\bxl$, $
        \Delta w_{j0} = \eta\sum_l(r_j^{(l)}-y_j^{(l))}$
    \paragraph{Regression for two-class Classification} $r^{(l)} = y^{(l)}+\epsilon$,
        $r^{(l)}\in \left\{ 0,1 \right\}, \epsilon \sim \cN(0, \sigma^2)$,
        $y^{(l)} = sigmoid(\bw^T\bxl+w_0)$,

        Likelihood:$L(\bw, w_0|\cX) =
        \prod_l\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[
        -\frac{(\rl-\yl)^2}{2\sigma^2} \right]$, 

        Error: $E(\left\{ \bw_i, w_{i0} \right\}_i|\cX) =
        \frac{1}{2}\sum_l(r^{(l)}-\yl)^2$, 

        Learning $\Delta \bw = \eta\sum_l(\rl - \yl)\yl(1-\yl)\bxl$, \\$\Delta
        w_0 = \eta\sum_l(\rl-\yl)\yl(1-\yl)$
    \paragraph{$K>2$ classes} $\brl = \byl +\boldsymbol\epsilon$, \\$\epsilon\sim
        \cN_K(0, \sigma^2\mathbf{I}_K)$, $\yil = \frac{1}{1+\exp\left[
            -(\bw_i^T\bxl + w_{i0})
        \right]}$, \\
        Likelihood:\\$L(\{\bw_i, w_{i0}\}_i|\cX) =
        \prod_l\frac{1}{(2\pi)^{K/2}|\bSig|^{1/2}}\exp\left[ -\frac{\|\brl -
        \byl\|^2}{2\sigma^2} \right]$\\
        Error Func: $E(\{\bw_i, w_{i0}\}|\cX) =
        \frac{1}{2}\sum_l\|\brl-\byl\|^2$\\
        Learning $\Delta \bw_i = \eta\sum_l(\rl_i - \yl_i)\yl_i(1-\yl_i)\bxl$,\\ $\Delta
        w_0 = \eta\sum_l(\rl_i-\yl_i)\yl_i(1-\yl_i)$
