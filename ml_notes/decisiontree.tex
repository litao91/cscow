\chapter{Decision Trees}
\section{Constructing Decision Trees}
\subsection{Basic alogrithm}
Can be expressed recursively. 
\begin{enumerate}
    \item select an attribute to place at the root node
        \begin{itemize}
            \item Make one branch for each possible value.
            \item This splits up the example set into subsets, one for
                each possible value.
        \end{itemize}
    \item Repeat the process recursively for each branch
    \item If at any tie all instances at a node have the same
        classification, stop.
\end{enumerate}

Only thing: how to determine which attribute to split on. 
\subsection{Measure of purity}
If we had a measure of the purity of each node, we could choose the
attribute that produces the purest daughter nodes.

We use \emph{information} measured with unit of \emph{bits}. 
\begin{itemize}
    \item It represents the expected amount of information that would be
        needed to specify whether a new instance should be classified
        yes or noo 
    \item Given the example reached that node.
    \item The value is often less than $1$
\end{itemize}
We calculate the information gained on different splits and choose the one
that gain most.
\subsubsection{Calculating Information}
Properties we expect to have:
\begin{itemize}
    \item When the number of either $yes$'s or $no$'s is zer, the
        information is zero
    \item When the number of $yes$'s and $no$'s equal, the information
        reaches a maximum
    \item The information should obey the multistage property that we
        have illustrated.
\end{itemize}
\subsubsection{Highly Branching Attributes}
The information gain measure tends to prefer attributes with large numbers
of possible values.

To compensate for this, a modification of the measure called the gain
ratio is widely used.  The gain raeio is derived by taking into account
the number and size of of daughter nodes into which an attribute splits
the dataset, disregarding any information about the class.

\section{Pruning}
Fully expanded decision trees often contain unnecessary structure, and it
is generally advisable to simplify them before they are deployed.

\begin{itemize}
    \item Prepruning would involve trying to decide during the tree
        building process when to stop developing subtrees. -- avoid all
        the work of developing subtrees only to throw them away afterward.
    \item Postpruning:situations occur in which attributes individually
        seem to have nothing to contribute but are powerful predictors
        when combined.
\end{itemize}
Two operations that have been considered for postpruning: \emph{subtree
replacement} and \emph{subtree raising}.

At each node, a learning scheme might decide whether it should perform
subtree replacement, subtree raising, or leave the subtree as it is,
unpruned.

\paragraph{Subtree replacement}
The idea is to select some subtrees and replace them with single leaves.
This will certainly cause the accuracy on the training set to decrease,
however, it may \textbf{increase the accuracy on an independently chosen
test set.}

When subtree replacement is implemented, it proceeds \textbf{from the leaves and
works back up toward the root.}
\paragraph{Subtree raising}
Replace an internal node by one of the nodes below it.

\subsection{Estimating Error Rates}
How to decide whether to replace an internal node by a leaf.

Estimate the error rate that would be expected at a particular node given
an independently chosen test set, at internal nodes and at leaf nodes.

If we had such an estimate, it would be clear whether to replace, or
raise. A particular subtree simply by comparing the estimated error of the
subtree with that of its proposed replacement.

Standard verification technique: Hold back some of the data originally
given and use it as an independent test set to estimate the error at each
node. Drawback: the actual tree is based on less data.

Alternative: try to make some stimate of error based on the training data
itself. It is a heuristic based on some statistical reasoning.

The idea is to consider the set of instances that reach each node and
imagine that the majority class is chosen to represent the node. That
gives us a certain number of errors, E, out of the total number of
instances, N.

Now imagine the true probability of error at the node is $q$ and that $N$
instances are generated by a Bernoulli process with parameter $q$, of
which $E$ turn out to be errors.

Given a particular confidence $c$, we find confidence limits $z$ such
that:
\[ 
    Pr\left[ \frac{f-q}{\sqrt{q(1-q)/N}} > z \right] = c
\]
Where $N$ is the number of samples, $f = E/N$ is the observed error rate,
and $q$ is the true error rate.

Now we use that upper confidence limit as a estimate for the error $e$ at
the node:
\[
    e = \frac{f+\frac{z^2}{2N}+z\sqrt{\frac{f}{N} -
    \frac{f^2}{N}+\frac{z^2}{4N^2}}}{1+\frac{z^2}{N}}
\]



\begin{description}
    \item[Classification Trees] for node m, $N_m$ training instances, $N_m^i$
        instances belong to class $C_i$, estimate for the probability of class
        $C_i$ $\hat{P}(C_i|\bx, m) = \frac{N^i_m}{N_m} = p^j_m$, pure if $p^j_m
        = 1$
    \item[Entropy] $\mathcal{T}_m = - \sum_{i=1}^K p_m^j\log_2p_m^j$, assume $0\log 0
        =0$, largest is $\log_2 K$ when all $p^i_m = 1/K$
    \item[Other Impurity Measures] Properties: $\phi(1/2,1/2) \geq \phi(p,1-p)$,
        $\phi(0,1) = \phi(1,0) = 0$, $\phi(p,1-p)$ increase in p on $[0, 1/2]$
        and decrease o$[1/2,1]$
    \item[Best Split] Node m, $N_{mj}$ take branch $j$, if $f_m(\bx)=j$, the
        estimate for the probability of class $C_i$ is: $\hat{P}(C_i|\bx, m, j)
        = p^i_{mj} = \frac{N^i_{mj}}{N_{mj}}$,total impurity after split:\\
        $ \mathcal{T}_m' = -\sum_{i=1}^n\frac{N_{mj}}{N_m}\sum_{i=1}^Kp^i_{mj}\log
        p^i_{mj} $
    \item[Regression Trees] $b_m(\bx) = 1$ if in $\cX_m$, \\estimated value at
        node m: \[g_m = \frac{\sum_l b_m(\bxl)\yl}{\sum_l b_m(\bxl)}\], \\mean
        square
        error after split: \[E_m = \frac{1}{N_m}\sum_l (\yl - g_m)^2 b_m
        (\bxl)\]
        tree expansion: \[b_{mj}(\bx) = 1 \mbox{if } \bx \in \cX_{mj}\], \\estimate
        val in branch $j$: $g_{mj} = \frac{\sum_l b_{mj}(\bxl)\yl}{\sum_l
            b_{mj}(\bxl)}$, \\error after split $E_m' =
            \frac{1}{N_m}\sum_j\sum_l(\yl - g_{mj}^2b_{mj}(\bxl)$\\
        Best split: split that resuts in smallest error, or worst possible
        error.
    \item[Pruning] Prepruning stop split when the number of instances reaching a
        node is belong a certain percentage. Post pruning: Replace subtree by a
        leaf node, if the leaf node does not perform worse, the subtree is pruned
        and replaced by leaf.

\end{description}
