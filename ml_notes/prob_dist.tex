\chapter{Probability Distributions}
Density estimation: model the probability distribution $p(\bx)$ of a
random variable $\bx$, given a finite set $\bx_1 \dots \bx_N$ of
observation.

Assumption: data points are independent and identically distributed
(i.i.d)

\begin{itemize}
    \item \emph{binomial} and \emph{multinomial} distribution for discrete
    \item \emph{Gaussian distribution} for continuous random variables.
\end{itemize}

These are specific examples of \emph{parametric} distribution because they
are governed by a samll number of adaptive parameters.

\begin{itemize}
    \item Frequentist treatment: choose specific value for the parameters
        by optimizing some criterion, such as likelihood function
    \item Bayesian treatment: \textbf{introduce prior
        distributions over the parameter}, and then use Baye's theorem to
        compute the corresponding posterior distribution given the
        observed data.
\end{itemize}

\section{Binary Variables}
(Flipping coins) $x \in \{0, 1\}$, $p(x=1|\mu) = \mu$, $p(x=0|\mu) = 1 -
\mu$, where $\mu$ is the parameter, the probability distribution:
\[
    \mbox{Bern}(x|\mu) = \mu^x{(1-\mu)}^{1-x}
\]
It's the \emph{Bernoulli} distribution.

The mean and variance are given by:
\begin{align*}
    \mathbb{E}[x] &= \mu \\
    \mbox{var}[x] &= \mu(1-\mu)
\end{align*}

Now dataset $\mathcal{D} = \left\{ x_1, \dots, x_N \right\}$. We can
construct the \textbf{likelihood function} (function of $\mu$). 

The likelihood function is the probability of drawing this dataset.

Assume that the data set is drawn from $p(x|\mu)$, the likelihood
function:

\[ 
    p(\mathcal{D}|\mu) = \prod_{n=1}^N p(x_n|\mu) = \prod_{n=1}^N
    \mu^{x_n}{(1-\mu)}^{1-x_n}
\]

In a frequentist setting, we can estimate a value for $\mu$ by maximizing
the likelihood function. Or the log likelihood:

\[
    \ln p(\mathcal{D}|\mu) = \sum_{n=1}^N \ln p(x_n|\mu) =
    \sum_{n=1}^N\left\{ x_n \ln \mu + (1-x_n)\ln(1-\mu) \right\}
\]

If we set the derivative of $\ln p(\mathcal{D}|\mu)$ w.r.t. $\mu$ equal to
zero, we have an maximum likelihood estimator:

\[
    \mu_{ML} = \frac{1}{N}\sum_{n=1}^N x_n
\]

It's the \emph{sample mean}

Binomial distribution: number $m$ of observations of $x=1$ given that the
data set has size $N$:

\begin{align*}
    \mbox{Bin}(m|N, \mu) = {N \choose m} \mu^m{(1-\mu)}^{N-m} \\
    \mathbb{E}[m] = \sum_{m=1}^N \mbox{mbox}(m|N, \mu) = N\mu \\
    \mbox{var}[m] = \sum_{m=0}^N {(m -\mathbb{E}[m])}^2 \mbox{Bin}(m|N,
    \mu) = N \mu (1-\mu)
\end{align*}

\subsection{The beta distribution}
Prior distribution $p(\mu)$ over the parameter $\mu$

Take the form of the products of factors of the form
$\mu^x{(1-\mu)}^{1-x}$

Choose a prior to be proportional to power of $\mu$ and $(1-\mu)$ then the
posterior will have the same functional form as the prior.

Posterior and prior have the same form.

The prior is \emph{beta} distribution:
\begin{align*}
    \mbox{Beta}(\mu|a, b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}
    \mu^{a-1}{(1-\mu)}^{b-1} \\
    \mathbb{E}[\mu] = \frac{a}{a+b}\\
    \mbox{var}[\mu] = \frac{ab}{{(a+b)}^2(a+b+1)}
\end{align*}

$a$ and $b$ are hyperparameters (parameters of parameters)

Then the posterior:
\[ p(\mu|m, l, a, b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{m+a-1} {(1-\mu)}^{l+b-1}\]

Simple interpretation of $a$ and $b$: effective number of observations of
$x=1$ and $x = 0$. (They add to the counts of samples).

\emph{Sequential} approach to learning:
\begin{enumerate}
    \item Taking observations one at a time
    \item After each observation updating the current posterior.
        Posterior is gotten by Multiplying by the likelihood ($P(\mathcal{D}|\theta)$) for the
        new observation, and then normalizing to obtain new posterior.
    \item At each stage, the posterior is a beta distribution with some
        total number of (prior plus actual) observed values for $x = 1$
        and $x = 0$, given by the parameter $a$ and $b$.
    \item Incorporation of an additional observation of $x = 1$ simply
        corresponds to incrementing the value of $a$ by $1$.
\end{enumerate}

Sequential methods make use of observations one at a time, or in small
batches, and then discard them befroe the next observations are used.

To predict:

\[ p(x = 1|\mathcal{D}) = \int_0^1 p(x = 1|\mu) p(\mu|\mathcal{D}) du =
    \int_0^1 \mu p(\mu|\mathcal{D}) du = \mathbb{E}[\mu|\mathcal{D}]\]

Using the result of the mean of beta distribution:

\begin{equation}
 p(x = 1|\mathcal{D}) = \frac{m+a}{m+a+l+b}
\end{equation}

The total fraction of observations (real obsercations and fictitious prior
observations) that correspond to $x = 1$.

Note that in the limit of an infinitely large $m, l \rightarrow \infty$
the result reduces to the maximum likelihood result.

\section{Multinomial Variables}
Variables ca take on one of $K$ possible mutually exclusive states.

Represented by a $K$-D vector $\bx$ in which one of $x_k = 1$. Denote the
probability of $x_k = 1$ by parameter $\mu_k$, then the distribution of
$\bx$:
\begin{equation}
    p(\bx|\bmu) = \prod_{k=1}^K \mu_k^{x_k}
\end{equation}

The likelihood
\begin{equation}
    P(\calD|\bmu) = \prod_{n=1}^K \prod_{k=1}^K \mu_k^{x_{nk}} =
    \prod_{k=1}^K \mu_k^{\sum_{n}x_{nk}} = \prod_{k=1}^K \mu_k^{m_k}
\end{equation}
The estimation can be achieved with the help of Lagrange:
\begin{equation}
    \mu_k^{\mbox{ML}} = \frac{m_k}{N}
\end{equation}

Multinominal distribution, the joint distribution of $m_1, \dots, m_K$
conditioned on the parameters $\bmu$ and $N$
\begin{equation}
    \mbox{Mult}(m_1,m_2, \dots, m_K|\bmu, N) = {N \choose m_1m_2\dots m_K}
    \prod_{k=1}^K \mu_k^{m_k}
    \label{multi-dist}
\end{equation}

Where 

\begin{equation}
    {N \choose m_1m_2\dots m_K} = \frac{N!}{m_1!m_2!\dots m_K!}
\end{equation}

\subsection{The Dirichlet distribution}
The conjugate prior:
\begin{equation}
    p(\bmu|\balpha) \propto \prod_{k=1}^K {\mu_k}^{\alpha_k -1}
\end{equation}

The normalized form:
\begin{equation}
    \mbox{Dir}(\bmu|\balpha) = \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)
    \cdots \Gamma(\alpha_K)} \prod_{k=1}^K \mu_k^{\alpha_k -1}
\end{equation}

Where $\Gamma(x)$ is the gamma function and $a_0 = \sum_{k=1}^K \alpha_k$.

\section{Gaussian Distribution}
The D-dimensional vector $\bx$, the multivariate Gaussian distribution
takes the form:

\begin{equation}
    \mathcal{N}(\bx|\bmu, \bSig) = \frac{1}{{(2\pi)}^{D/2}}
    \frac{1}{{|\bSig|}^{1/2}} \exp\left\{ -\frac{1}{2} {(\bx -
    \bmu)}^T\bSig^{-1}(\bx - \bmu) \right\} 
    \label{gaussian}
\end{equation}
Where $\mu$ is a $D$-dimensional mean vector, $\bSig$ is a $D \times D$
covariance matrix, and $|\bSig|$ denotes the determinant of $\bSig$.

\subsection{Partitioned Gaussians}
Given a joint Gaussian distribution $\mathcal{N}(\bx|\bmu, \bSig)$ with
$\boldsymbol\Lambda \equiv \bSig^{-1}$ and 
\begin{align}
    \bx = \begin{pmatrix}\bx_a \\ \bx_b \end{pmatrix}, \quad \bmu =
    \begin{pmatrix}\bmu_a, \bmu_b
    \end{pmatrix} \\
    \bSig = \begin{pmatrix}\bSig_{aa} & \bSig_{ab} \\ \bSig_{ba} &
        \bSig_{bb} \end{pmatrix} , \quad 
    \bLam = \begin{pmatrix} \bLam_{aa} & \bLam_{ab} \\ \bLam_{ba} &
        \bLam_{bb} \end{pmatrix}
\end{align}

Conditional distribution:
\begin{align}
    p(\bx_a|\bx_b) &= \mathcal{N}(\bx|\bmu_{a|b}, \bLam^{-1}_{aa}) \\
    \bum_{a|b} &= \mu_a - \bLam^{-1}_{aa}\bLam_{ab}(\bx_b -
    \bmu_{b})
\end{align}
